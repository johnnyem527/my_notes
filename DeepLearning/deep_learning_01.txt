MLP（Multi-layer Perceptron）: 多层神经网络，多层感知器，是一种前向结构的人工神经网络
MLP的核心是BP算法

深度学习：其实是BP算法优化

机器学习：想让计算机像人一样思考所研发出来的计算机理论。
系统：
    信息传递
    存储单元

机器学习诞生于19世纪60年代，跨学科的交融（概率论，统计学，生物学等）
运用：
    百度搜索图片识别
    google now
    google photo
    google alphaGo
    汇率预测
    房价涨跌预测
    
实现机器学习的方法通常叫做算法。
所有的机器学习算法大致被分为5类
    1. 监督学习：数据，数据对应的值（标签信息）（如:神经网络，股票预测，房价预测）
    2. 无监督学习：根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习
    3. 半监督学习：少量有标签的样本，和大量无标签的样本
    4. 强化学习：把机器人丢在一个陌生的环境，让它自己适应，但是需要给提示，比如成功记一分
    5. 遗传算法：进化理论，优胜劣汰

机器学习（按目的分类）
    判别模型：决策函数y=f(x)或条件概率分布P(Y|X)作为预测模型，即判别模型，通过有限样本条件下建立判别函数
    生成模型：由数据学习联合概率密度分布P(X,Y), 用来创造新的东西    

神经网络：人小时候神经元没有连接形成神经网络
         人学习的时候，神经元就会通过神经节点连接起来形成神经网络
         连接断裂就会遗忘

前向计算---->得到结果，得到h
    对比结果正确与否之后，进行后向计算
后向计算---->学习（调整参数），方法有最小二值法（问题：只能做两种对象的分类），梯度下降法（问题：梯度离散+局部最优解）

激活函数：被激活函数激活的神经元作为重点对待对象

调整激活函数，优化器就是在调整激活函数，调整激活函数就是在调整梯度，用什么方法梯度下降法

层数越多，学习越难，甚至学不下去了（如梯度下降中的梯度离散，局部最优解问题）

人脑：（并行运算）
    突触；连接其他神经元
    树突：接受信息
    轴突：发送信息，传输信息的
    神经元内部是通过电脉冲来传递数据
    神经元之间是通过神经递质来传递数据的

计算机神经元（串行运算）
    树突（输入）：x1, x2, x3, x4
    轴突（发送数据）
    h = wx + b (w是权重，因为树突长短不同，h代表当前神经元输出的结果)
    加偏置b是为了升维（站在矩阵的角度看）
    h = w1x1 + w2x2 + w3x3
    h = WX(W, X都是矩阵)

样本：(xy),x代表的是输入的值，y代表的是预期的目标（标签）
损失函数(loss)：(y-h)^2 或 |y-h| 表示预期目标和实际目标的差值, 越小说明网络越好，越大说明网络越不行
输入：x1, x2, x3, x4
输出：h(x) = f(wx)
净输出：未被激活前的输出，用来判断当前的网络会不会死掉！
激活：h = f(wx)：这里的 f就是激活函数，加激活函数是为了让感知机具有非线性性


线性可分
线性不可分
或运算 可用h=wx解决
与运算 可用h=wx解决
异或运算 不能用h=wx解决，只能用h=wx+b解决

激活函数的作用：
    h = f(wx + b)
    提供非线性能力
    增强鲁棒性

两种激活函数：sigmoid (定义域: -oo ~ +oo, 值域:0~1), tanh (值域-1 ~ 1)
    sigmoid：求概率问题时可用
    tanh：判别类型的问题可用
    要求处处可微分
    值域时-1~1或者0~1，定义域负无穷到正无穷，定义域在-1~1之间梯度最大，其他地方梯度很小，为的是增加学习速度

梯度（导数->偏导->方向导数->梯度）：
    梯度是众多方向导数中最大的那个向量，这个方向就用梯度来表示（grad=ai+bj）这个向量来表示，
    其中a是函数在x方向上的偏导数，b是函数在y方向上的偏导数，梯度的模就是这个最大方向导数的值。

收敛<=>离散

归一化(Normalization):顾名思义把数据化为0~1之间的数，为了让样本表现的比较规律，方法就是除以样本的最大的值域
去均值：有些时候归一化之后还会减去0.5，让样本变得更小一点
